{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e255a79e-d963-47d5-8958-c5037c552b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import hashlib\n",
    "import ipaddress\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import socket\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Literal, Optional, Union\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import aiofiles\n",
    "import censys\n",
    "import lief\n",
    "import pandas as pd\n",
    "import tldextract\n",
    "import yara\n",
    "\n",
    "# https://github.com/censys/censys-python\n",
    "# https://github.com/censys/censys-python\n",
    "from censys.search import CensysCerts, CensysHosts\n",
    "from oletools import rtfobj\n",
    "from oletools.oleid import OleID\n",
    "from oletools.olevba import VBA_Parser\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "NON_COMMERCIAL_API_LIMIT = 1000\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "exif_tool_path = \"bins/exiftool.exe\"\n",
    "floss_executable_path = \"bins/floss2.2.exe\"\n",
    "\n",
    "\n",
    "top_500_domains = \"data/top_500_domains.csv\"\n",
    "censys_api_id = os.getenv(\"CENSYS_API_ID\")\n",
    "censys_api_secret = os.getenv(\"CENSYS_API_SECRET\")\n",
    "censys_certificates = CensysCerts(api_id=censys_api_id, api_secret=censys_api_secret)\n",
    "censys_hosts = CensysHosts(api_id=censys_api_id, api_secret=censys_api_secret)\n",
    "\n",
    "\n",
    "def get_first_submission_date(vt_meta: dict[str, Any]):\n",
    "    first_submission_date = vt_meta[\"attributes\"][\"first_submission_date\"]\n",
    "    return datetime.utcfromtimestamp(first_submission_date)\n",
    "\n",
    "\n",
    "def extract_ole_features(\n",
    "    metadata: dict[str, Any],\n",
    "    output_directory: str,\n",
    "    filename: str = \"oletool_features_updated.json\",\n",
    ") -> None:\n",
    "    \"\"\"Extracts the OLE features from a given file and writes them to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        metadata (dict): The metadata of the file.\n",
    "        output_directory (str): The directory to write the JSON file to.\n",
    "        filename (str, optional): The name of the JSON file to write the OLE features to. Defaults to \"oletool_features_updated.json\".\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(output_directory, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    ole_features = defaultdict(dict)\n",
    "    sample_hash = os.path.basename(os.path.normpath(output_directory))\n",
    "    file_type = metadata.get(\"FileType\", \"\")\n",
    "\n",
    "    if not is_supported_file_type(file_type):\n",
    "        logging.info(f\"Unsupported file type for {output_directory}.\")\n",
    "        return\n",
    "    file_to_process = os.path.join(output_directory, sample_hash)\n",
    "    try:\n",
    "        ole_features[sample_hash] = extract_features_from_file(\n",
    "            file_to_process, file_type=file_type\n",
    "        )\n",
    "        if ole_features[sample_hash]:\n",
    "            write_features_to_file(ole_features, file_path)\n",
    "    except Exception as e:\n",
    "        logging.info(f\"Exception occurred for sample {output_directory}: {e}\")\n",
    "        if ole_features:\n",
    "            write_features_to_file(ole_features, file_path)\n",
    "\n",
    "\n",
    "def is_supported_file_type(file_type: str) -> bool:\n",
    "    \"\"\"Checks if the file type is supported for OLE feature extraction.\n",
    "\n",
    "    Args:\n",
    "        file_type (str): The file type of the file based on exif.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file type is supported, False otherwise.\n",
    "    \"\"\"\n",
    "    supported_types = [\n",
    "        \"DOC\",\n",
    "        \"DOCX\",\n",
    "        \"DOTM\",\n",
    "        \"PPT\",\n",
    "        \"PDF\",\n",
    "        \"RTF\",\n",
    "        \"XLS\",\n",
    "        \"XLSX\",\n",
    "        \"FPX\",\n",
    "        \"ZIP\",\n",
    "    ]\n",
    "    return file_type in supported_types\n",
    "\n",
    "\n",
    "def extract_features_from_file(file: str, file_type: str) -> dict[str, Any]:\n",
    "    \"\"\"Extracts features from a file based on its metadata.\n",
    "\n",
    "    Args:\n",
    "        file (str): The path to the file.\n",
    "        file_type (str): The file type of the file based on exif.\n",
    "\n",
    "    Returns:\n",
    "        dict: The features extracted from the file.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    if file_type == \"RTF\":\n",
    "        features.update(extract_rtf_features(file))\n",
    "    if file_type == \"PDF\":\n",
    "        features.update(pdf_feature(file))\n",
    "    if file_type not in [\"RTF\", \"PDF\"]:\n",
    "        oid = OleID(file)\n",
    "        indicators = oid.check()\n",
    "        for indicator in indicators:\n",
    "            if isinstance(indicator.value, (bytes, bytearray)):\n",
    "                features[indicator.name] = indicator.value.decode(\"latin-1\")\n",
    "            else:\n",
    "                features[indicator.name] = indicator.value\n",
    "\n",
    "        vba_features = extract_vba_features(file)\n",
    "        for key in vba_features:\n",
    "            features[key] = vba_features.get(key)\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_rtf_features(file: str) -> dict[str, str]:\n",
    "    \"\"\"Extracts features from an RTF file.\n",
    "    Args:\n",
    "        file (str): The path to the RTF file.\n",
    "    Returns:\n",
    "        dict: The features extracted from the RTF file.\n",
    "    \"\"\"\n",
    "    features = {\"rtfobject\": {}}\n",
    "    for index, orig_len, data in rtfobj.rtf_iter_objects(file):\n",
    "        features[\"rtfobject\"][hex(index)] = f\"size {len(data)}\"\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_vba_features(file: str) -> dict[str, Any]:\n",
    "    \"\"\"Extracts VBA features from a file.\n",
    "\n",
    "    Args:\n",
    "        file (str): The path to the file.\n",
    "    Returns:\n",
    "        dict: The VBA features extracted from the file.\n",
    "    \"\"\"\n",
    "    vbaparser = VBA_Parser(file)\n",
    "    if not vbaparser.detect_vba_macros():\n",
    "        logging.info(f\"The file {file} doesn't have VBA macros.\")\n",
    "        return {}\n",
    "\n",
    "    features = {\"VBAMacro\": defaultdict(list)}\n",
    "    for filename, stream_path, vba_filename, vba_code in vbaparser.extract_macros():\n",
    "        features[\"VBAMacro\"][\"Filename\"].append(filename)\n",
    "        features[\"VBAMacro\"][\"OLEstream\"].append(stream_path)\n",
    "        features[\"VBAMacro\"][\"VBAfilename\"].append(vba_filename)\n",
    "        features[\"VBAMacro\"][\"VBAcode\"].append(vba_code)\n",
    "\n",
    "    results = vbaparser.analyze_macros()\n",
    "    for kw_type, keyword, description in results:\n",
    "        features[\"VBAMacro\"][\"Keyword Types\"].append(kw_type)\n",
    "        if features.get(\"Keyword Found\"):\n",
    "            features[\"Keyword Found\"].setdefault(keyword, description)\n",
    "        else:\n",
    "            features[\"Keyword Found\"] = {keyword: description}\n",
    "\n",
    "    features[\"VBAMacro\"].update(\n",
    "        {\n",
    "            \"AutoExec keywords\": vbaparser.nb_autoexec,\n",
    "            \"Suspicious keywords\": vbaparser.nb_suspicious,\n",
    "            \"IOCs\": vbaparser.nb_iocs,\n",
    "            \"Hex obfuscated strings\": vbaparser.nb_hexstrings,\n",
    "            \"Base64 obfuscated strings\": vbaparser.nb_base64strings,\n",
    "            \"Dridex obfuscated strings\": vbaparser.nb_dridexstrings,\n",
    "            \"VBA obfuscated strings\": vbaparser.nb_vbastrings,\n",
    "        }\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "def write_features_to_file(features, file_path) -> None:\n",
    "    \"\"\"Writes the extracted features to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        features (dict): The extracted features.\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w+\") as f:\n",
    "        json.dump(features, f, indent=4)\n",
    "\n",
    "\n",
    "def extract_pdf_metadata(reader):\n",
    "    \"\"\"Extracts metadata from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        reader (PdfReader): A PdfReader object of the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the PDF metadata.\n",
    "    \"\"\"\n",
    "    metadata = reader.metadata\n",
    "    return {\n",
    "        \"Number of Pages\": len(reader.pages),\n",
    "        \"Author\": getattr(metadata, \"author\", None),\n",
    "        \"Creator\": getattr(metadata, \"creator\", None),\n",
    "        \"Producer\": getattr(metadata, \"producer\", None),\n",
    "        \"Subject\": getattr(metadata, \"subject\", None),\n",
    "        \"Title\": getattr(metadata, \"title\", None),\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_pdf_text(page):\n",
    "    \"\"\"Extracts text from the first page of a PDF file.\n",
    "\n",
    "    Args:\n",
    "        page (PageObject): The first page object of the PDF.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the first page of the PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return page.extract_text() or \"No text found\"\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to extract text: {e}\")\n",
    "        return \"Failed to extract text\"\n",
    "\n",
    "\n",
    "def pdf_feature(file_name):\n",
    "    \"\"\"Extracts features from a PDF file including metadata and text from the first page.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        defaultdict: A dictionary containing extracted features and information.\n",
    "    \"\"\"\n",
    "    feature_set = defaultdict(dict)\n",
    "\n",
    "    try:\n",
    "        reader = PdfReader(file_name)\n",
    "        first_page = reader.pages[0] if reader.pages else None\n",
    "\n",
    "        if first_page:\n",
    "            feature_set[\"Page\"][\"Text\"] = extract_pdf_text(first_page)\n",
    "\n",
    "        feature_set[\"Information\"] = extract_pdf_metadata(reader)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Warning: Exception occurred {e}\")\n",
    "        feature_set[\"Exception\"] = str(e)\n",
    "\n",
    "    return feature_set\n",
    "\n",
    "\n",
    "def get_exiftool_json(\n",
    "    file_path: str, parsing_charset: str = \"latin1\"\n",
    ") -> Optional[Union[dict[str, Any], None]]:\n",
    "    \"\"\"\n",
    "    Get Exiftool output in JSON format for a given file with a specific charset (Latin-1).\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the file.\n",
    "        parsing_charset: What encoding to process the file with\n",
    "\n",
    "    Returns:\n",
    "        dict or None: A dictionary containing the Exiftool information in JSON format,\n",
    "                    or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        exiftool_command = [\n",
    "            exif_tool_path,\n",
    "            \"-j\",\n",
    "            \"-charset\",\n",
    "            parsing_charset,\n",
    "            file_path,\n",
    "        ]\n",
    "        exiftool_output = subprocess.check_output(\n",
    "            exiftool_command, universal_newlines=True, encoding=parsing_charset\n",
    "        )\n",
    "        exif_data = json.loads(exiftool_output)[0]\n",
    "        return exif_data\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise e\n",
    "        logging.debug(f\"Error handling output from subprocess: {e}\")\n",
    "        return None\n",
    "    except Exception as all_other_exceptions:\n",
    "        raise all_other_exceptions\n",
    "        logging.debug(f\"Error running Exiftool: {all_other_exceptions}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_document_file(file_path: str, output_directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Process a file to extract OLE features and write them to a JSON file.\n",
    "\n",
    "    Examples:\n",
    "        >>> process_document_file(r\"C:/users/randomuser/5aaaaaaaaabbb/5aaaaaaaaabbb\", r\"C:/users/randomuser/5aaaaaaaaabbb\")\n",
    "        >>> process_document_file(r\"C:/users/randomuser/6aaa8845ssscc/6aaa8845ssscc\", r\"C:/users/randomuser/6aaa8845ssscc\")\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file.\n",
    "        output_directory (str): The directory to write the JSON file to.\n",
    "    \"\"\"\n",
    "    metadata = get_exiftool_json(file_path)\n",
    "    if metadata:\n",
    "        extract_ole_features(metadata, output_directory)\n",
    "\n",
    "\n",
    "def process_yara_rule(\n",
    "    file_path: str,\n",
    "    yara_rules_path: str,\n",
    "    output_dir: str = None,\n",
    "    result_filename: str = \"malcatYaraResults.json\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Applies YARA rules to a specified file and saves the matches in a JSON file.\n",
    "\n",
    "    Examples:\n",
    "        >>> process_yara_rule(\"C:/users/randomuser/5aaaaaaaaabbb/5aaaaaaaaabbb\", \"yara_rules/malcat.yar\", \"C:/users/randomuser/5aaaaaaaaabbb\")\n",
    "        >>> process_yara_rule(\"C:/users/randomuser/6aaa8845ssscc/6aaa8845ssscc\", \"yara_rules/malcat.yar\", \"C:/users/randomuser/6aaa8845ssscc\")\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the file to be scanned.\n",
    "        yara_rules_path: Path to the YARA rules file.\n",
    "        output_dir: Directory to save the results JSON file to.\n",
    "        result_filename: Name of the JSON file to save the results to. Defaults to \"malcatYaraResults.json\".\n",
    "    \"\"\"\n",
    "    # Compile YARA rules from the specified file path\n",
    "    rules = yara.compile(\n",
    "        filepath=yara_rules_path, includes=True, error_on_warning=False\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        matches = rules.match(file_path)\n",
    "        yara_match = {\"hash\": os.path.basename(os.path.normpath(file_path))}\n",
    "\n",
    "        # Store each match in the dictionary\n",
    "        for match in matches:\n",
    "            yara_match[str(match)] = True\n",
    "\n",
    "        # Determine the path for the results JSON file\n",
    "        result_path = os.path.join(output_dir, result_filename)\n",
    "\n",
    "        # Write the matches to the specified JSON file\n",
    "        with open(result_path, \"w+\") as f:\n",
    "            json.dump(yara_match, f, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"Error extracting the yara rules for the file path {file_path}: {e}\"\n",
    "        )\n",
    "\n",
    "\n",
    "async def process_floss_file(\n",
    "    file_path: str,\n",
    "    output_directory: str,\n",
    "    floss_executable: str = \"floss2.2.exe\",\n",
    "    out_file_name: str = \"flossresults_reduced_7.json\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Executes FLOSS on a specified file and saves the output to a JSON file.\n",
    "\n",
    "    Examples:\n",
    "        asyncio.run(process_floss_file(r\"C:\\\\Users\\\\ExampleUser\\\\Documents\\\\ThreatReportSamples\\\\sample.exe\",\n",
    "                           r\"C:\\\\Users\\\\ExampleUser\\\\Documents\\\\OutputDirectory\",\n",
    "                           \"bins/floss2.2.exe\",\n",
    "                           \"flossresults_reduced_7.json\"))\n",
    "\n",
    "    Args:\n",
    "        file_path: The full path to the file to be processed.\n",
    "        output_directory: The directory where the output file should be saved.\n",
    "        floss_executable: The path to the FLOSS executable. Defaults to \"floss2.2.exe\".\n",
    "        out_file_name: The name of the output JSON file. Defaults to \"flossresults_reduced_7.json\".\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        logging.error(f\"File does not exist: {file_path}\")\n",
    "        return\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    path_to_out_file = os.path.join(output_directory, out_file_name)\n",
    "    command = f'powershell {floss_executable} --json -n 7 -o \"{path_to_out_file}\" --no stack tight decoded -- \"{file_path}\"'\n",
    "\n",
    "    process = await asyncio.create_subprocess_shell(\n",
    "        command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE\n",
    "    )\n",
    "\n",
    "    _, stderr = await process.communicate()\n",
    "    if stderr:\n",
    "        logging.error(f\"Error while processing floss on file: {file_path}: {stderr}\")\n",
    "\n",
    "    logging.info(f\"Command output for file {os.path.basename(file_path)}:\")\n",
    "\n",
    "\n",
    "def process_exiftool(\n",
    "    file_path: str,\n",
    "    output_directory: str,\n",
    "    encoding: str = \"latin1\",\n",
    "    output_file_name: str = \"exiftool_results.json\",\n",
    ") -> Optional[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process a file to extract metadata using Exiftool and write it to a JSON file.\n",
    "\n",
    "    Examples:\n",
    "        >>> process_exiftool(r\"C:/users/randomuser/5aaaaaaaaabbb/5aaaaaaaaabbb\", r\"C:/users/randomuser/5aaaaaaaaabbb\")\n",
    "        >>> process_exiftool(r\"C:/users/randomuser/6aaa8845ssscc/6aaa8845ssscc\", r\"C:/users/randomuser/6aaa8845ssscc\")\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file.\n",
    "        output_directory (str): The directory to write the JSON file to.\n",
    "        encoding (str): The encoding to use when processing the file. Defaults to \"latin1\".\n",
    "        output_file_name (str): The name of the JSON file to write the Exiftool results to. Defaults to \"exiftool_results.json\".\n",
    "    \"\"\"\n",
    "    metadata = get_exiftool_json(file_path, encoding)\n",
    "    if metadata:\n",
    "        write_features_to_file(\n",
    "            metadata, os.path.join(output_directory, output_file_name)\n",
    "        )\n",
    "    return metadata\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import lief\n",
    "\n",
    "\n",
    "def from_timestamp_to_date(timestamp: int) -> str:\n",
    "    \"\"\"\n",
    "    Converts a timestamp into a printable date string.\n",
    "\n",
    "    Args:\n",
    "        timestamp (int): Timestamp to be converted.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted date string (e.g., \"Jan 01 2019 at 00:00:00\").\n",
    "    \"\"\"\n",
    "    if not timestamp:\n",
    "        return None\n",
    "    return datetime.utcfromtimestamp(timestamp).strftime(\"%b %d %Y at %H:%M:%S\")\n",
    "\n",
    "\n",
    "def extract_common_attributes(binary: Any, feature_set: dict, file_type: str):\n",
    "    \"\"\"\n",
    "    Extracts common attributes from the binary file and updates the feature set to include\n",
    "    platform, CPU type, file type, entrypoint, and the number of sections.\n",
    "\n",
    "    This version is compatible with PE, ELF, and Mach-O binary types, accommodating the differences\n",
    "    in their attribute names and structures.\n",
    "\n",
    "    Args:\n",
    "        binary: The binary file being processed, expected to be an instance from lief.\n",
    "        feature_set (defaultdict[list]): The feature set to update with the extracted attributes.\n",
    "        file_type (str): The type of the file (e.g., 'PE', 'ELF', 'MachO').\n",
    "    \"\"\"\n",
    "    feature_set[\"Platform\"] = file_type\n",
    "\n",
    "    # Handling CPU type variations\n",
    "    cpu_type_attr = (\n",
    "        getattr(binary.header, \"cpu_type\", None)\n",
    "        or getattr(binary.header, \"machine\", None)\n",
    "        or getattr(binary.header, \"machine_type\", None)\n",
    "    )\n",
    "    feature_set[\"CPU type\"] = str(cpu_type_attr) if cpu_type_attr else \"Unknown\"\n",
    "\n",
    "    # Handling file type if applicable\n",
    "    file_type_attr = getattr(binary.header, \"file_type\", None)\n",
    "    feature_set[\"File type\"] = str(file_type_attr) if file_type_attr else \"Unknown\"\n",
    "\n",
    "    # Entrypoint, if available\n",
    "    if hasattr(binary, \"entrypoint\"):\n",
    "        feature_set[\"Entrypoint\"] = binary.entrypoint\n",
    "\n",
    "    # Number of sections\n",
    "    number_of_sections_attr = getattr(binary.header, \"numberof_sections\", None)\n",
    "    if number_of_sections_attr is not None:\n",
    "        feature_set[\"Number of sections\"] = number_of_sections_attr\n",
    "\n",
    "\n",
    "def lief_header(binary, file_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Displays header information for ELF, PE, and Mach-O files.\n",
    "\n",
    "    Args:\n",
    "        binary: The binary file to process.\n",
    "        file_type (str): The type of the binary file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted features.\n",
    "    \"\"\"\n",
    "    feature_set = dict()\n",
    "    extract_common_attributes(binary, feature_set, file_type)\n",
    "\n",
    "    if file_type == \"machofile\":\n",
    "        feature_set.update(extract_macho_attributes(binary))\n",
    "    if file_type in [\"EXEfile\", \"DLLfile\"]:\n",
    "        feature_set.update(extract_pe_attributes(binary))\n",
    "    if file_type == \"elffile\":\n",
    "        feature_set.update(extract_elf_attributes(binary))\n",
    "\n",
    "    if not feature_set:\n",
    "        print(\"Warning: No header found for the specified file type.\")\n",
    "    return feature_set\n",
    "\n",
    "\n",
    "def extract_macho_attributes(binary: lief.MachO.Binary) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts Mach-O specific attributes from the binary file.\n",
    "\n",
    "    Args:\n",
    "        binary: The Mach-O binary file to process.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the extracted Mach-O attributes.\n",
    "    \"\"\"\n",
    "    feature_set = {}\n",
    "    feature_set[\"CPU type\"] = str(binary.header.cpu_type)\n",
    "    feature_set[\"File type\"] = str(binary.header.file_type)\n",
    "    feature_set[\"Number of commands\"] = binary.header.nb_cmds\n",
    "    feature_set[\"Size of commands\"] = binary.header.sizeof_cmds\n",
    "    feature_set[\"Flags\"] = \":\".join(str(flag) for flag in binary.header.flags_list)\n",
    "    return feature_set\n",
    "\n",
    "\n",
    "def resolve_attribute_value(value: Any) -> Union[dict, list, str, int, float, None]:\n",
    "    \"\"\"Attempts to resolve the value of attributes, handling specific types to include names and values in a dict.\n",
    "\n",
    "    Args:\n",
    "        value: The value to resolve.\n",
    "\n",
    "    Returns:\n",
    "        The resolved value, which could be a primitive type, string representation, a dictionary, or a list of dictionaries.\n",
    "    \"\"\"\n",
    "    if isinstance(value, (list, set)):\n",
    "        # Handle lists or sets, assuming they contain objects that can be resolved to name-value pairs\n",
    "        return [\n",
    "            {item.name: item.value}\n",
    "            for item in value\n",
    "            if hasattr(item, \"name\") and hasattr(item, \"value\")\n",
    "        ]\n",
    "    elif hasattr(value, \"name\") and hasattr(value, \"value\"):\n",
    "        # Handle single objects that can be resolved to name-value pairs\n",
    "        return {value.name: value.value}\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "def extract_pe_attributes(binary: lief.PE.Binary) -> dict:\n",
    "    \"\"\"Extracts PE specific attributes from the binary file, handling specific complex types.\n",
    "\n",
    "    Args:\n",
    "        binary: The PE binary file to process.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the extracted PE attributes, properly handling complex or enumerable types.\n",
    "    \"\"\"\n",
    "    feature_set = {\n",
    "        \"Date of compilation\": from_timestamp_to_date(binary.header.time_date_stamps),\n",
    "        \"Imphash\": lief.PE.get_imphash(binary),\n",
    "    }\n",
    "\n",
    "    # Handle optional header specific attributes, resolving complex or enumerable types\n",
    "    if binary.header.sizeof_optional_header > 0:\n",
    "        optional_header_attrs = {}\n",
    "        for key in dir(binary.optional_header):\n",
    "            if not key.startswith(\"_\"):  # Skip private attributes\n",
    "                raw_value = getattr(binary.optional_header, key, None)\n",
    "                value = resolve_attribute_value(raw_value)\n",
    "                # Check to ensure the value is serializable (now including dicts and lists of dicts)\n",
    "                if isinstance(value, (str, int, float, dict, list)):\n",
    "                    optional_header_attrs[key] = value\n",
    "        feature_set[\"Optional header\"] = optional_header_attrs\n",
    "\n",
    "    return feature_set\n",
    "\n",
    "\n",
    "def extract_elf_attributes(binary: lief.ELF.Binary) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts ELF-specific attributes from an ELF binary file.\n",
    "\n",
    "    Args:\n",
    "        binary: The ELF binary file being processed.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the extracted ELF attributes.\n",
    "    \"\"\"\n",
    "    feature_set = {}\n",
    "\n",
    "    # Direct attribute extraction with straightforward mapping\n",
    "    feature_set[\"Platform\"] = \"ELF\"\n",
    "    feature_set[\"Magic\"] = bytes(binary.header.identity).hex()\n",
    "    feature_set[\"Type\"] = str(binary.header.file_type)\n",
    "    feature_set[\"Entrypoint\"] = hex(binary.header.entrypoint)\n",
    "    feature_set[\"ImageBase\"] = hex(binary.imagebase) if binary.imagebase else \"-\"\n",
    "    feature_set[\"Header size\"] = binary.header.header_size\n",
    "    feature_set[\"Endianness\"] = str(binary.header.identity_data)\n",
    "    feature_set[\"Class\"] = str(binary.header.identity_class)\n",
    "    feature_set[\"OS/ABI\"] = str(binary.header.identity_os_abi)\n",
    "    feature_set[\"Version\"] = str(binary.header.identity_version)\n",
    "    feature_set[\"Architecture\"] = str(binary.header.machine_type)\n",
    "\n",
    "    # Handling ELF-specific flags, such as MIPS Flags, if applicable\n",
    "    if hasattr(binary.header, \"mips_flags_list\") and binary.header.mips_flags_list:\n",
    "        mips_flags = \":\".join(str(flag) for flag in binary.header.mips_flags_list)\n",
    "    else:\n",
    "        mips_flags = \"No flags\"\n",
    "    feature_set[\"MIPS Flags\"] = mips_flags\n",
    "\n",
    "    # Additional ELF-specific details\n",
    "    feature_set[\"Number of sections\"] = binary.header.numberof_sections\n",
    "    feature_set[\"Number of segments\"] = binary.header.numberof_segments\n",
    "    feature_set[\"Program header offset\"] = hex(binary.header.program_header_offset)\n",
    "    feature_set[\"Program header size\"] = binary.header.program_header_size\n",
    "    feature_set[\"Section Header offset\"] = hex(binary.header.section_header_offset)\n",
    "    feature_set[\"Section header size\"] = binary.header.section_header_size\n",
    "\n",
    "    return feature_set\n",
    "\n",
    "\n",
    "def check_binary_format(\n",
    "    binary,\n",
    ") -> Literal[\"DLLfile\", \"EXEfile\", \"machofile\", \"elffile\", \"null\"]:\n",
    "    \"\"\"\n",
    "    Checks the format of a binary file and returns its type as a literal string.\n",
    "\n",
    "    Args:\n",
    "        binary: The binary file to check.\n",
    "\n",
    "    Returns:\n",
    "        Literal['DLLfile', 'EXEfile', 'machofile', 'elffile', 'null']: A literal string indicating the type of the binary.\n",
    "    \"\"\"\n",
    "    if not binary:\n",
    "        return \"null\"\n",
    "\n",
    "    if binary.format == lief.EXE_FORMATS.PE:\n",
    "        if binary.header.characteristics & lief.PE.HEADER_CHARACTERISTICS.DLL:\n",
    "            return \"DLLfile\"\n",
    "        else:\n",
    "            return \"EXEfile\"\n",
    "    elif binary.format == lief.EXE_FORMATS.MACHO:\n",
    "        return \"machofile\"\n",
    "    elif binary.format == lief.EXE_FORMATS.ELF:\n",
    "        return \"elffile\"\n",
    "\n",
    "    return \"null\"\n",
    "\n",
    "\n",
    "def exported_functions(binary: lief.Binary, file_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts exported functions from ELF, PE, Mach-O binaries.\n",
    "\n",
    "    Args:\n",
    "        binary: The binary file being processed.\n",
    "        file_type: The type of the binary file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing exported functions if any.\n",
    "    \"\"\"\n",
    "    feature_set = {\"Exported functions\": []}\n",
    "    if binary.exported_functions:\n",
    "        for function in binary.exported_functions:\n",
    "            feature_set[\"Exported functions\"].append(str(function.name))\n",
    "    else:\n",
    "        logger.info(\"Warning: No exported function found\")\n",
    "\n",
    "    return feature_set\n",
    "\n",
    "\n",
    "def imported_functions(binary: lief.Binary, file_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts imported functions from ELF, PE, Mach-O binaries.\n",
    "\n",
    "    Args:\n",
    "        binary: The binary file being processed.\n",
    "        file_type: The type of the binary file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing imported functions if any.\n",
    "    \"\"\"\n",
    "    feature_set = {\"Imported functions\": []}\n",
    "    if binary.imported_functions:\n",
    "        for function in binary.imported_functions:\n",
    "            feature_set[\"Imported functions\"].append(str(function.name))\n",
    "    else:\n",
    "        logger.info(\"Warning: No imported function found\")\n",
    "\n",
    "    return feature_set\n",
    "\n",
    "\n",
    "def print_elf_symbols(symbols: list, title: str) -> dict:\n",
    "    \"\"\"\n",
    "    Formats ELF symbols for display.\n",
    "\n",
    "    Args:\n",
    "        symbols: List of symbols.\n",
    "        title: Title for the display section.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing formatted ELF symbols.\n",
    "    \"\"\"\n",
    "    feature_set_sub = {\n",
    "        title: {\n",
    "            \"Name\": [],\n",
    "        }\n",
    "    }\n",
    "    if symbols:\n",
    "        for symbol in symbols:\n",
    "            feature_set_sub[title][\"Name\"].append(symbol.name)\n",
    "    else:\n",
    "        logger.info(f\"Warning: No {title.lower()} found\")\n",
    "\n",
    "    return feature_set_sub\n",
    "\n",
    "\n",
    "def exported_symbols(binary: lief.Binary, file_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts exported symbols from ELF, Mach-O binaries.\n",
    "\n",
    "    Args:\n",
    "        binary: The binary file being processed.\n",
    "        file_type: The type of the binary file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing exported symbols if any.\n",
    "    \"\"\"\n",
    "    feature_set = {}\n",
    "    if file_type == \"elffile\" and binary.exported_symbols:\n",
    "        feature_set = print_elf_symbols(binary.exported_symbols, \"Exported symbols\")\n",
    "    elif file_type == \"machofile\" and binary.exported_symbols:\n",
    "        feature_set[\"Exported symbols\"] = {\n",
    "            \"Name\": [],\n",
    "        }\n",
    "        for symbol in binary.exported_symbols:\n",
    "            feature_set[\"Exported symbols\"][\"Name\"].append(symbol.name)\n",
    "    else:\n",
    "        logger.info(\"Warning: No exported symbol found\")\n",
    "\n",
    "    return feature_set\n",
    "\n",
    "\n",
    "def imported_symbols(binary: lief.Binary, file_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts imported symbols from ELF, Mach-O binaries.\n",
    "\n",
    "    Args:\n",
    "        binary: The binary file being processed.\n",
    "        file_type: The type of the binary file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing imported symbols if any.\n",
    "    \"\"\"\n",
    "    feature_set = {}\n",
    "    if file_type == \"elffile\" and binary.imported_symbols:\n",
    "        feature_set = print_elf_symbols(binary.imported_symbols, \"Imported symbols\")\n",
    "    elif file_type == \"machofile\" and binary.imported_symbols:\n",
    "        feature_set[\"Imported symbols\"] = {\n",
    "            \"Name\": [],\n",
    "            \"Number of sections\": [],\n",
    "            \"Value\": [],\n",
    "            \"Origin\": [],\n",
    "        }\n",
    "        for symbol in binary.imported_symbols:\n",
    "            feature_set[\"Imported symbols\"][\"Name\"].append(symbol.name)\n",
    "            feature_set[\"Imported symbols\"][\"Number of sections\"].append(\n",
    "                symbol.numberof_sections\n",
    "            )\n",
    "            feature_set[\"Imported symbols\"][\"Value\"].append(hex(symbol.value))\n",
    "            feature_set[\"Imported symbols\"][\"Origin\"].append(str(symbol.origin))\n",
    "    else:\n",
    "        logger.info(\"Warning: No imported symbols found\")\n",
    "\n",
    "    return feature_set\n",
    "\n",
    "\n",
    "def resources(binary: lief.PE.Binary, file_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts PE resources, if any, from the binary.\n",
    "\n",
    "    Args:\n",
    "        binary: The binary file being processed.\n",
    "        file_type: The type of the binary file, expecting 'EXEfile' or 'DLLfile'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing resources information if available.\n",
    "    \"\"\"\n",
    "    feature_set = {}\n",
    "    if file_type in [\"EXEfile\", \"DLLfile\"] and binary.has_resources:\n",
    "        resource_type = (\n",
    "            \"Directory\"\n",
    "            if binary.resources.is_directory\n",
    "            else \"Data\" if binary.resources.is_data else \"Unknown\"\n",
    "        )\n",
    "        feature_set[\"Resources\"] = {\n",
    "            \"Name\": binary.resources.name if binary.resources.has_name else \"No name\",\n",
    "            \"Number of childs\": len(binary.resources.childs),\n",
    "            \"Depth\": binary.resources.depth,\n",
    "            \"Type\": resource_type,\n",
    "            \"Id\": hex(binary.resources.id),\n",
    "        }\n",
    "\n",
    "        resource_manager = {}\n",
    "        if binary.resources_manager.has_type:\n",
    "            resource_manager[\"Type\"] = \", \".join(\n",
    "                str(rType) for rType in binary.resources_manager.types_available\n",
    "            )\n",
    "\n",
    "        if binary.resources_manager.langs_available:\n",
    "            langs_available = \", \".join(\n",
    "                str(lang) for lang in binary.resources_manager.langs_available\n",
    "            )\n",
    "            sublangs_available = \", \".join(\n",
    "                str(sublang) for sublang in binary.resources_manager.sublangs_available\n",
    "            )\n",
    "            resource_manager.update(\n",
    "                {\"Language\": langs_available, \"Sub-language\": sublangs_available}\n",
    "            )\n",
    "\n",
    "        if resource_manager:\n",
    "            feature_set[\"Resource manager\"] = resource_manager\n",
    "    else:\n",
    "        logger.info(\"Warning: No resource found\")\n",
    "\n",
    "    return feature_set\n",
    "\n",
    "\n",
    "def dlls(binary: lief.PE.Binary, file_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Lists the DLLs imported by the PE binary.\n",
    "\n",
    "    Args:\n",
    "        binary: The binary file being processed.\n",
    "        file_type: The type of the binary file, expecting 'EXEfile' or 'DLLfile'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing a list of imported libraries if available.\n",
    "    \"\"\"\n",
    "    if file_type in [\"EXEfile\", \"DLLfile\"] and binary.libraries:\n",
    "        return {\"Libraries\": binary.libraries}\n",
    "    else:\n",
    "        logger.info(\"Error: No dll found\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def imports(binary: lief.PE.Binary, file_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts import information from the PE binary.\n",
    "\n",
    "    Args:\n",
    "        binary: The binary file being processed.\n",
    "        file_type: The type of the binary file, expecting 'EXEfile' or 'DLLfile'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing imports names and functions if available.\n",
    "    \"\"\"\n",
    "    feature_set = {\n",
    "        \"Imports Name\": [],\n",
    "        \"Imports Function IAT\": [],\n",
    "        \"Imports Function name\": [],\n",
    "    }\n",
    "    if file_type in [\"EXEfile\", \"DLLfile\"] and binary.imports:\n",
    "        for imp in binary.imports:\n",
    "            feature_set[\"Imports Name\"].append(imp.name)\n",
    "            for function in imp.entries:\n",
    "                feature_set[\"Imports Function IAT\"].append(hex(function.iat_address))\n",
    "                feature_set[\"Imports Function name\"].append(function.name)\n",
    "    else:\n",
    "        logger.info(\"Warning: No import found\")\n",
    "\n",
    "    return feature_set\n",
    "\n",
    "\n",
    "def load_configuration(binary: lief.PE.Binary, file_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts load configuration details from the PE binary.\n",
    "\n",
    "    Args:\n",
    "        binary: The binary file being processed.\n",
    "        file_type: The type of the binary file, expecting 'EXEfile' or 'DLLfile'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing load configuration details if available.\n",
    "    \"\"\"\n",
    "    if file_type in [\"EXEfile\", \"DLLfile\"] and binary.has_configuration:\n",
    "        config = binary.load_configuration\n",
    "        return {\n",
    "            \"Configuration\": {\n",
    "                \"Version\": str(config.version),\n",
    "                \"Characteristics\": hex(config.characteristics),\n",
    "                \"Timedatestamp\": from_timestamp_to_date(config.timedatestamp),\n",
    "                \"Major version\": config.major_version,\n",
    "                \"Minor version\": config.minor_version,\n",
    "                \"Security cookie\": hex(config.security_cookie),\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        logger.info(\"Warning: No load configuration found\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def signature(pe: lief.PE.Binary, type_of_binary: str) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts and displays the PE signature information, if available.\n",
    "\n",
    "    Args:\n",
    "        pe: The PE binary file being processed.\n",
    "        type_of_binary: The type of the binary file, expecting 'EXEfile' or 'DLLfile'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the signature information if available.\n",
    "    \"\"\"\n",
    "    feature_set = {}\n",
    "    if type_of_binary in [\"EXEfile\", \"DLLfile\"] and pe.has_signatures:\n",
    "        feature_set[\"Signature\"] = {\n",
    "            \"MD5 authentihash\": pe.authentihash_md5.hex(),\n",
    "            \"SHA1 authentihash\": pe.authentihash(lief.PE.ALGORITHMS.SHA_1).hex(),\n",
    "        }\n",
    "\n",
    "        # Extract signer details from the first signature\n",
    "        if pe.signatures:\n",
    "            cert_signer = pe.signatures[0].signers[0].cert\n",
    "            signer_details = {\n",
    "                line.split(\" : \")[0].strip(): line.split(\" : \")[1].strip()\n",
    "                for line in str(cert_signer).split(\"\\n\")\n",
    "                if \" : \" in line\n",
    "            }\n",
    "            feature_set[\"Signature\"][\"Signer details\"] = signer_details\n",
    "\n",
    "    else:\n",
    "        # Assuming a logging mechanism or similar feedback for when signatures are not found\n",
    "        print(\"Warning: No signature found for the specified binary type.\")\n",
    "\n",
    "    return feature_set\n",
    "\n",
    "\n",
    "def get_sections(binary: lief.Binary, type_of_binary: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts and displays the sections of ELF, PE, Mach-O binaries.\n",
    "\n",
    "    Args:\n",
    "        binary: The binary file being processed.\n",
    "        type_of_binary: The type of the binary file ('elffile', 'EXEfile', 'DLLfile', 'machofile').\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with sections' details if available.\n",
    "    \"\"\"\n",
    "    sections_info = {\"Sections\": {}}\n",
    "    if not binary.sections:\n",
    "        print(\"Warning: No section found\")\n",
    "        return sections_info\n",
    "\n",
    "    rows = []\n",
    "    if type_of_binary == \"elffile\":\n",
    "        for section in binary.sections:\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"Name\": section.name,\n",
    "                    \"Offset\": hex(section.offset),\n",
    "                    \"Virtual address\": hex(section.virtual_address),\n",
    "                    \"Size\": f\"{section.size:<6} bytes\",\n",
    "                    \"Type\": str(section.type),\n",
    "                    \"Flags\": \":\".join(str(flag) for flag in section.flags_list),\n",
    "                    \"Entropy\": round(section.entropy, 4),\n",
    "                }\n",
    "            )\n",
    "    elif type_of_binary in [\"EXEfile\", \"DLLfile\"]:\n",
    "        for section in binary.sections:\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"Name\": section.name,\n",
    "                    \"Virtual address\": hex(section.virtual_address),\n",
    "                    \"Virtual size\": f\"{section.virtual_size:<6} bytes\",\n",
    "                    \"Offset\": hex(section.offset),\n",
    "                    \"Size\": f\"{section.size:<6} bytes\",\n",
    "                    \"Entropy\": round(section.entropy, 4),\n",
    "                }\n",
    "            )\n",
    "    elif type_of_binary == \"machofile\":\n",
    "        for section in binary.sections:\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"Name\": section.name,\n",
    "                    \"Virtual address\": hex(section.virtual_address),\n",
    "                    \"Type\": str(section.type),\n",
    "                    \"Size\": f\"{section.size:<6} bytes\",\n",
    "                    \"Offset\": hex(section.offset),\n",
    "                    \"Entropy\": round(section.entropy, 4),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Updating sections info after processing all sections\n",
    "    if rows:  # If rows list is not empty, update sections_info\n",
    "        sections_info[\"Sections\"] = rows\n",
    "\n",
    "    return sections_info\n",
    "\n",
    "\n",
    "def code_signature(binary: lief.MachO.Binary, type_of_binary: str) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Extracts and displays the Mach-O code signature if available.\n",
    "\n",
    "    Args:\n",
    "      binary: The Mach-O binary file being processed.\n",
    "      type_of_binary: The type of the binary file, expected to be 'machofile'.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary containing code signature details if available.\n",
    "    \"\"\"\n",
    "    if type_of_binary == \"machofile\" and binary.has_code_signature:\n",
    "        return {\n",
    "            \"Code signature\": {\n",
    "                \"Command\": str(binary.code_signature.command),\n",
    "                \"Command offset\": hex(binary.code_signature.command_offset),\n",
    "                \"Command size\": f\"{binary.code_signature.size:<6} bytes\",\n",
    "                \"Data offset\": hex(binary.code_signature.data_offset),\n",
    "                \"Data size\": f\"{binary.code_signature.data_size:<6} bytes\",\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        logger.info(\"Warning: No code signature found\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def source_version(binary: lief.MachO.Binary, type_of_binary: str) -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Displays the Mach-O source version if available.\n",
    "\n",
    "    Args:\n",
    "      binary: The Mach-O binary file being processed.\n",
    "      type_of_binary: The type of the binary file, expected to be 'machofile'.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary containing source version details if available.\n",
    "    \"\"\"\n",
    "    if type_of_binary == \"machofile\" and binary.has_source_version:\n",
    "        return {\n",
    "            \"Source version\": {\n",
    "                \"Command\": str(binary.source_version.command),\n",
    "                \"Offset\": hex(binary.source_version.command_offset),\n",
    "                \"Size\": binary.source_version.size,\n",
    "                \"Version\": list_version_to_dotted_version(\n",
    "                    binary.source_version.version\n",
    "                ),\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        logger.info(\"Warning: No source version found\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def list_version_to_dotted_version(version_list: list[int]) -> str:\n",
    "    \"\"\"\n",
    "    Converts a version represented as a list into a dotted string representation.\n",
    "\n",
    "    Args:\n",
    "      version_list: List of version values.\n",
    "\n",
    "    Returns:\n",
    "      A formatted version string in the format '0.0.0.0....'\n",
    "    \"\"\"\n",
    "    return \".\".join(str(v) for v in version_list) if version_list else None\n",
    "\n",
    "\n",
    "def interpreter(binary: lief.ELF.Binary, type_of_binary: str) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Displays the interpreter for ELF binaries if available.\n",
    "\n",
    "    Args:\n",
    "      binary: The ELF binary file being processed.\n",
    "      type_of_binary: The type of the binary file, expected to be 'elffile'.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary containing the interpreter path if available.\n",
    "    \"\"\"\n",
    "    if type_of_binary == \"elffile\" and binary.has_interpreter:\n",
    "        return {\"Interpreter\": binary.interpreter}\n",
    "    else:\n",
    "        logger.info(\"Warning: No interpreter found\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def lief_features(subdir: str, sample: str) -> None:\n",
    "    \"\"\"\n",
    "    Analyzes a binary file with LIEF and extracts various features, saving them as a JSON file.\n",
    "\n",
    "    Args:\n",
    "        subdir: The directory where to save the features file.\n",
    "        sample: The path to the binary sample to analyze.\n",
    "    \"\"\"\n",
    "    final_result = {}\n",
    "\n",
    "    try:\n",
    "        binary = lief.parse(sample)\n",
    "        type_of_binary = check_binary_format(\n",
    "            binary\n",
    "        )  # Updated to use the refactored function name\n",
    "\n",
    "        # Assuming each function returns a dictionary and takes `binary` and `type_of_binary` as arguments\n",
    "        functions = [\n",
    "            lief_header,  # Assuming this combines common and type-specific header info\n",
    "            code_signature,\n",
    "            source_version,\n",
    "            signature,\n",
    "            interpreter,\n",
    "            get_sections,\n",
    "            imported_functions,\n",
    "            exported_functions,\n",
    "            imported_symbols,\n",
    "            exported_symbols,\n",
    "            resources,\n",
    "            dlls,\n",
    "            imports,\n",
    "            load_configuration,\n",
    "        ]\n",
    "\n",
    "        # Iterating over each function and updating the final_result dictionary\n",
    "        for func in functions:\n",
    "            result = func(binary, type_of_binary)\n",
    "            final_result.update(result)\n",
    "\n",
    "        # Writing the results to a JSON file\n",
    "        filename = \"lief_features.json\"\n",
    "        path_to_file = os.path.join(subdir, filename)\n",
    "        with open(path_to_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(final_result, f, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(\"An exception occurred during the feature extraction: %s\", e)\n",
    "\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def sha256(data: bytes) -> bytes:\n",
    "    \"\"\"\n",
    "    Generates SHA-256 hash of the given data.\n",
    "\n",
    "    Args:\n",
    "        data: Data to hash.\n",
    "\n",
    "    Returns:\n",
    "        SHA-256 hash of the data.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(data).digest()\n",
    "\n",
    "\n",
    "def decode_base58(bc: str, length: int) -> bytes:\n",
    "    \"\"\"\n",
    "    Decodes a base58-encoded string.\n",
    "\n",
    "    Args:\n",
    "        bc: Base58 encoded string.\n",
    "        length: Expected length of the decoded data.\n",
    "\n",
    "    Returns:\n",
    "        Decoded data as bytes.\n",
    "    \"\"\"\n",
    "    digits58 = \"123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz\"\n",
    "    n = 0\n",
    "    for char in bc:\n",
    "        n = n * 58 + digits58.index(char)\n",
    "    return n.to_bytes(length, \"big\")\n",
    "\n",
    "\n",
    "def check_bc(bc: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a given Bitcoin address is valid.\n",
    "\n",
    "    Args:\n",
    "        bc: Bitcoin address.\n",
    "\n",
    "    Returns:\n",
    "        True if valid, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bcbytes = decode_base58(bc, 25)\n",
    "        return bcbytes[-4:] == sha256(sha256(bcbytes[:-4]).digest()).digest()[:4]\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "import ipaddress\n",
    "\n",
    "\n",
    "def is_valid_and_public_ip(ip: str) -> bool:\n",
    "    \"\"\"\n",
    "    Validates an IP address and checks if it is public (globally routable).\n",
    "\n",
    "    Args:\n",
    "        ip: The IP address to validate.\n",
    "\n",
    "    Returns:\n",
    "        True if the IP address is valid and public, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ip_obj = ipaddress.ip_address(ip)\n",
    "        return (\n",
    "            ip_obj.is_global\n",
    "            and not ip_obj.is_reserved\n",
    "            and not ip_obj.is_private\n",
    "            and not ip_obj.is_multicast\n",
    "            and not ip_obj.is_unspecified\n",
    "            and not ip_obj.is_loopback\n",
    "            and not ip_obj.is_link_local\n",
    "        )\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_regex_patterns() -> dict:\n",
    "    \"\"\"\n",
    "    Defines regular expressions for various patterns.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of pattern names to their corresponding regular expressions.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"URL\": r\"(http[s]?|ftp|telnet|ldap|file):\\/\\/([\\w|\\d]{2,6}\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(\\:[\\d]{2,6})?\",\n",
    "        \"IPv4\": r\"(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\",\n",
    "        \"MD5\": r\"[a-fA-F0-9]{32}\",\n",
    "        \"SHA1\": r\"[a-fA-F0-9]{40}\",\n",
    "        \"SHA256\": r\"[a-fA-F0-9]{64}\",\n",
    "        \"WindowsFilePath\": r\"(?:[\\w]\\:|\\\\)(\\\\[a-z_\\-\\s0-9\\.]+)+\\.(txt|gif|pdf|doc|docx|xls|xlsx|msg|log|rtf|key|dat|jpg|png|exe|bat|apk|jar|js|php|htm|html|dll|lnk)\",\n",
    "        \"LinuxFilePath\": r\"\\/[\\w]{3,10}[\\/]+[\\w]{1,40}[\\/]+([\\w|+|-|%|\\.|~|_|-|\\/])*[\\w|+|-|%|\\.|~|_|-]{1,255}\",\n",
    "        \"Ethereum\": r\"^0x[a-fA-F0-9]{40}$\",\n",
    "        \"Bitcoin\": r\"([13]|bc1)[A-HJ-NP-Za-km-z1-9]{25,39}\",\n",
    "        \"Email\": r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6}\",\n",
    "        \"SlackToken\": r\"(xox[pboa]-[0-9]{12}-[0-9]{12}-[0-9]{12}-[a-z0-9]{32})\",\n",
    "        \"RSAprivatekey\": r\"-----BEGIN RSA PRIVATE KEY-----\",\n",
    "        \"SSHDSAprivatekey\": r\"-----BEGIN DSA PRIVATE KEY-----\",\n",
    "        \"SSHECprivatekey\": r\"-----BEGIN EC PRIVATE KEY-----\",\n",
    "        \"PGPprivatekeyblock\": r\"-----BEGIN PGP PRIVATE KEY BLOCK-----\",\n",
    "        \"GitHub\": r\"[gG][iI][tT][hH][uU][bB].*['|\\\"][0-9a-zA-Z]{35,40}['|\\\"]\",\n",
    "        \"GenericAPIKey\": r\"[aA][pP][iI]_?[kK][eE][yY].*['|\\\"][0-9a-zA-Z]{32,45}['|\\\"]\",\n",
    "        \"GoogleAPIKey\": r\"AIza[0-9A-Za-z\\\\-_]{35}\",\n",
    "        \"GoogleGCPServiceaccount\": r\"\\\"type\\\": \\\"service_account\\\"\",\n",
    "        \"GoogleGmailAPIKey\": r\"AIza[0-9A-Za-z\\\\-_]{35}\",\n",
    "        \"GoogleGmailOAuth\": r\"[0-9]+-[0-9A-Za-z_]{32}\\\\.apps\\\\.googleusercontent\\\\.com\",\n",
    "        \"PayPalBraintreeAccessToken\": r\"access_token\\\\$production\\\\$[0-9a-z]{16}\\\\$[0-9a-f]{32}\",\n",
    "        \"TwitterAccessToken\": r\"[tT][wW][iI][tT][tT][eE][rR].*[1-9][0-9]+-[0-9a-zA-Z]{40}\",\n",
    "        \"TwitterOAuth\": r\"[tT][wW][iI][tT][tT][eE][rR].*['|\\\"][0-9a-zA-Z]{35,44}['|\\\"]\",\n",
    "    }\n",
    "\n",
    "\n",
    "def combine_patterns(patterns: dict[str, str]) -> re.Pattern:\n",
    "    \"\"\"\n",
    "    Combines multiple regex patterns into a single pattern with named groups.\n",
    "\n",
    "    Args:\n",
    "        patterns: A dictionary of patterns to combine, where keys are pattern names.\n",
    "\n",
    "    Returns:\n",
    "        A compiled regex object of the combined pattern.\n",
    "    \"\"\"\n",
    "    combined_pattern = \"|\".join(\n",
    "        [f\"(?P<{name}>{pattern})\" for name, pattern in patterns.items()]\n",
    "    )\n",
    "    return re.compile(combined_pattern, re.MULTILINE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "def extract_matches_combined(\n",
    "    text: str, combined_regex: re.Pattern\n",
    ") -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Extracts matches for named groups in the combined regex pattern from the given text. This implementation has better\n",
    "    performance than extract_with_regex_individual_patterns. However, due to the greedy approach of regular expressions, we will\n",
    "    miss true positive events.\n",
    "\n",
    "    Args:\n",
    "        text: The text to search through.\n",
    "        combined_regex: The compiled combined regex pattern with named groups.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where each key is a pattern name and the value is a list of unique matches for that pattern.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for match in combined_regex.finditer(text):\n",
    "        for name, value in match.groupdict().items():\n",
    "            if value:  # If there's a match for this named group\n",
    "                if name not in results:\n",
    "                    results[name] = [value]\n",
    "                elif value not in results[name]:\n",
    "                    results[name].append(value)\n",
    "    return results\n",
    "\n",
    "\n",
    "def find_pattern_matches(text: str, pattern: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Finds all matches of a single pattern in the given text.\n",
    "\n",
    "    Args:\n",
    "        text: The text to search through.\n",
    "        pattern: The regex pattern to apply.\n",
    "\n",
    "    Returns:\n",
    "        A list of unique matches for the pattern.\n",
    "    \"\"\"\n",
    "    return list(\n",
    "        {\n",
    "            match.group(0)\n",
    "            for match in re.finditer(pattern, text, re.MULTILINE | re.IGNORECASE)\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "async def async_extract_with_regex_individual_patterns(text: str) -> Dict[str, list]:\n",
    "    \"\"\"\n",
    "    Asynchronously applies each regex pattern individually to the provided text.\n",
    "    \"\"\"\n",
    "    patterns = (\n",
    "        get_regex_patterns()\n",
    "    )  # Assuming this function is defined elsewhere and returns a dict of patterns\n",
    "    results = {}\n",
    "\n",
    "    # Process each pattern asynchronously\n",
    "    for name, pattern in patterns.items():\n",
    "        matches = set()\n",
    "        for match in re.finditer(pattern, text, re.MULTILINE | re.IGNORECASE):\n",
    "            matched_text = match.group(0)\n",
    "            if not matched_text:\n",
    "                continue\n",
    "            # Skip invalid IP addresses\n",
    "            if name in [\"IPv4\", \"IPv6\"] and not is_valid_and_public_ip(matched_text):\n",
    "                continue\n",
    "            matches.add(matched_text)\n",
    "        results[name] = list(matches)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "async def regex_fun(\n",
    "    path_to_json: str,\n",
    "    file_hash: str,\n",
    "    subdir: str,\n",
    "    reprocess: bool = False,\n",
    "    file_name: str = \"regex_results.json\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Asynchronously extracts various patterns like URLs, file paths, cryptographic hashes, and more from a given file.\n",
    "    \"\"\"\n",
    "    result_filename = os.path.join(subdir, file_name)\n",
    "\n",
    "    if os.path.exists(result_filename) and not reprocess:\n",
    "        logger.info(\"Result file already exists: %s\", result_filename)\n",
    "        return\n",
    "\n",
    "    results = {file_hash: {}}\n",
    "\n",
    "    try:\n",
    "        async with aiofiles.open(path_to_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = await f.read()\n",
    "\n",
    "        data = json.loads(data)\n",
    "        all_strings = data.get(\"strings\", {})\n",
    "        if not all_strings:\n",
    "            raise ValueError(\n",
    "                f\"Error: Could not find strings in the provided file '{path_to_json}'\"\n",
    "            )\n",
    "\n",
    "        raw_static_strings = \" \".join(\n",
    "            s.get(\"string\").strip()\n",
    "            for s in all_strings.get(\"static_strings\", [])\n",
    "            if s.get(\"string\")\n",
    "        )\n",
    "        details = await async_extract_with_regex_individual_patterns(raw_static_strings)\n",
    "        results[file_hash] = details\n",
    "\n",
    "        async with aiofiles.open(result_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            await f.write(json.dumps(results, indent=4))\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(\n",
    "            \"Exception occurred while processing file %s: %s\", path_to_json, e\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "def censys_ip_data(ip: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches host data for a given IP address from Censys.\n",
    "\n",
    "    Args:\n",
    "      ip: The IP address to query host data for.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary representing the host data for the given IP address.\n",
    "    \"\"\"\n",
    "    if ip is None:\n",
    "        return {}\n",
    "    try:\n",
    "        host = censys_hosts.view(ip)\n",
    "        return host\n",
    "    except Exception as e:\n",
    "        # Log error or handle it as per your logging setup\n",
    "        print(f\"Error fetching data for IP {ip}: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def censys_host_data(domain_name: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetches host data for a given domain name from Censys.\n",
    "\n",
    "    Args:\n",
    "      domain_name: The domain name to query host data for.\n",
    "\n",
    "    Returns:\n",
    "      A list of dictionaries, each representing the host data for the domain.\n",
    "    \"\"\"\n",
    "    domain_host_result = []\n",
    "    censys_host_result = censys_hosts.search(domain_name, max_records=10)\n",
    "    for search_result_host in censys_host_result:\n",
    "        domain_host_result.append(search_result_host)\n",
    "\n",
    "    return domain_host_result\n",
    "\n",
    "\n",
    "def censys_certificate_data(\n",
    "    domain_name: str, sample_left_date: datetime, sample_right_date: str = \"*\"\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Fetches certificate data for a given domain name within a specified date range from Censys.\n",
    "\n",
    "    Args:\n",
    "      domain_name: The domain name to query certificate data for.\n",
    "      sample_left_date: The start date for the query range.\n",
    "      sample_right_date: The end date for the query range, defaults to \"*\".\n",
    "\n",
    "    Returns:\n",
    "      A list of dictionaries, each representing the certificate data for the domain within the given date range.\n",
    "    \"\"\"\n",
    "    sample_left_date_str = sample_left_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    certificate_query = f\"parsed.extensions.subject_alt_name.dns_names:{domain_name} AND added_at:[{sample_left_date_str} TO {sample_right_date}]\"\n",
    "    certificates_search_results = censys_certificates.search(\n",
    "        certificate_query,\n",
    "        fields=[\n",
    "            \"parsed.subject.common_name\",\n",
    "            \"parsed.extensions.subject_alt_name.dns_names\",\n",
    "            \"parsed.issuer_dn\",\n",
    "            \"fingerprint_sha256\",\n",
    "            \"parsed.issuer.organization\",\n",
    "            \"validation.microsoft.in_revocation_set\",\n",
    "            \"validation.chrome.in_revocation_set\",\n",
    "            \"revocation.crl.revoked\",\n",
    "        ],\n",
    "        max_records=6,\n",
    "    )\n",
    "\n",
    "    domain_cert_data = []\n",
    "    for search_results in certificates_search_results:\n",
    "        if not search_results:\n",
    "            continue\n",
    "        search_result = search_results[0]\n",
    "\n",
    "        cert_data_fields = {\n",
    "            \"subdomains\": search_result.get(\n",
    "                \"parsed.extensions.subject_alt_name.dns_names\", []\n",
    "            ),\n",
    "            \"issuer_dn\": search_result.get(\"parsed.issuer_dn\", \"\"),\n",
    "            \"fingerprint_sha256\": search_result.get(\"fingerprint_sha256\", \"\"),\n",
    "            \"issuer_organization\": next(\n",
    "                iter(search_result.get(\"parsed.issuer.organization\", [])), None\n",
    "            ),\n",
    "            \"microsoft_banned\": search_result.get(\n",
    "                \"validation.microsoft.in_revocation_set\", False\n",
    "            ),\n",
    "            \"google_banned\": search_result.get(\n",
    "                \"validation.chrome.in_revocation_set\", False\n",
    "            ),\n",
    "            \"cert_revoked\": search_result.get(\"revocation.crl.revoked\", False),\n",
    "        }\n",
    "        domain_cert_data.append(cert_data_fields)\n",
    "\n",
    "    return domain_cert_data\n",
    "\n",
    "\n",
    "def process_censys_file(\n",
    "    regex_json_path: str, vt_meta_file_path: str, file_hash: str\n",
    ") -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Processes files to extract Censys features based on regex results and VirusTotal metadata.\n",
    "\n",
    "    Args:\n",
    "      regex_json_path: Path to the JSON file containing regex results.\n",
    "      vt_meta_file_path: Path to the JSON file containing VirusTotal metadata.\n",
    "      file_hash: The hash of the file to process.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary with Censys features for each URL and IP address found.\n",
    "    \"\"\"\n",
    "    with open(regex_json_path, \"r\") as file:\n",
    "        regex_results = json.load(file).get(file_hash, {})\n",
    "\n",
    "    with open(vt_meta_file_path, \"r\") as file:\n",
    "        vt_meta_json = json.load(file)\n",
    "\n",
    "    first_submission = get_first_submission_date(\n",
    "        vt_meta_json\n",
    "    )  # Assumes implementation elsewhere\n",
    "    return censys_features(regex_results, vt_meta_json)\n",
    "\n",
    "\n",
    "def censys_features(\n",
    "    regex_results: Dict[str, Any], vt_meta_json: Dict[str, Any]\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Fetches Censys features for the given URLs and IP addresses extracted via regex, along with meta information from VirusTotal.\n",
    "\n",
    "    Args:\n",
    "      regex_results: A dictionary containing regex-extracted values, including URLs and IP addresses.\n",
    "      vt_meta_json: A dictionary containing metadata from VirusTotal.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary with Censys features for each URL and IP address.\n",
    "\n",
    "    Example:\n",
    "        >> process_censys_file(regex_json_path, vt_meta_file_path, file_hash)\n",
    "    \"\"\"\n",
    "    censys_results = {}\n",
    "    urls = regex_results.get(\"URL\", [])\n",
    "    ips = regex_results.get(\"IPv4\", [])\n",
    "    first_submission = get_first_submission_date(vt_meta_json)\n",
    "    censys_urls = domain_info(urls, list_of_popular_domain)\n",
    "    # Process URLs\n",
    "    for parsed_url in censys_urls:\n",
    "\n",
    "        try:\n",
    "            censys_results[parsed_url] = {\"CertificateData\": [], \"DomainData\": []}\n",
    "            if first_submission:\n",
    "                certificate_data = censys_certificate_data(parsed_url, first_submission)\n",
    "                censys_results[parsed_url][\"CertificateData\"] = certificate_data\n",
    "            censys_results[parsed_url][\"DomainData\"] = censys_host_data(parsed_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception processing URL {parsed_url}: {e}\")\n",
    "\n",
    "    # Process IPs\n",
    "    for ip in ips:\n",
    "        try:\n",
    "            socket.inet_aton(ip)  # Validates IPv4 format\n",
    "            censys_results[ip] = {\"IPData\": censys_ip_data(ip)}\n",
    "        except socket.error:\n",
    "            print(f\"Invalid IP address format: {ip}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Exception processing IP address {ip}: {e}\")\n",
    "\n",
    "    return censys_results\n",
    "\n",
    "\n",
    "def get_popular_domains() -> list[str]:\n",
    "    \"\"\"\n",
    "    Reads a CSV file containing top domains and extracts unique root domains.\n",
    "\n",
    "    Returns:\n",
    "        A list of unique root domain names.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(top_500_domains)[\"Root Domain\"].unique().tolist()\n",
    "\n",
    "\n",
    "def domain_info(url_list: list[str], popular_domains: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Filters out URLs that belong to popular domains and extracts the domain part\n",
    "    from the remaining URLs.\n",
    "\n",
    "    Args:\n",
    "        url_list: A list of URLs to process.\n",
    "        popular_domains: A list of popular domain names to exclude.\n",
    "\n",
    "    Returns:\n",
    "        A list of unique domain names excluding popular domains and possible port numbers.\n",
    "    \"\"\"\n",
    "    censys_url: set[str] = set()\n",
    "\n",
    "    for url in url_list:\n",
    "        if url is None:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            domain_address = urlparse(url).netloc\n",
    "            # Split domain from possible port number\n",
    "            domain, _, _ = domain_address.partition(\":\")\n",
    "            # Check for second-level domain (SLD)\n",
    "            sld = \".\".join(domain.split(\".\")[-2:])\n",
    "            if domain not in popular_domains and sld not in popular_domains:\n",
    "                censys_url.add(domain)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred while processing URL {url}: {e}\")\n",
    "\n",
    "    return list(censys_url)\n",
    "\n",
    "\n",
    "# To run the async function, use asyncio.run() if calling from a non-async context\n",
    "# Example:\n",
    "# asyncio.run(regex_fun('/path/to/json', 'file_hash_example', '/sub/dir'))\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# macho\n",
    "# file_hash = \"3f5fb5fd5b29a805f13611c9e8acad3ce3e319c6060d92b693378a8506714e9c\"\n",
    "# elf packed\n",
    "# file_hash = \"469aa49f4f628498111af193d9220fcc41825d94525246656e40b0560d4cd267\"\n",
    "# elf no packing\n",
    "# file_hash = \"8b6380534dcae5830e1e194f8c54466db365246cb8df998686f04818e37d84c1\"\n",
    "# pe binary\n",
    "# file_hash = \"00d1726e2ba77c4bed66a6c5c7f1a743cf7bb480deff15f034d67cf72d558c83\"\n",
    "# dll\n",
    "file_hash = \"f4e9e73265a87139b9f090ff69d8c246ca68d858952f7eef0f0686ef24c5fbb6\"\n",
    "# file_hash = \"00bc6fcfa82a693db4d7c1c9d5f4c3d0bfbbd0806e122f1fbded034eb9a67b10\"\n",
    "\n",
    "root_dir = rf\"C:\\Users\\ricewater\\Documents\\TestCorpus\\{file_hash}\"\n",
    "floss_json_results = \"flossresults_reduced_7.json\"\n",
    "regex_file_name = \"regex_results.json\"\n",
    "sample_file_path = os.path.join(root_dir, file_hash)\n",
    "floss_json_path = os.path.join(root_dir, floss_json_results)\n",
    "regex_json_path = os.path.join(root_dir, regex_file_name)\n",
    "vt_meta_file = f\"{file_hash}.json\"\n",
    "vt_meta_file_path = os.path.join(root_dir, vt_meta_file)\n",
    "list_of_popular_domain = get_popular_domains()\n",
    "\n",
    "# # Run the process_file function with asyncio's event loop\n",
    "# asyncio.run(process_floss_file(sample_file_path, root_dir, floss_executable_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "668ba68b-512d-4829-a05b-b63887a54ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %autoawait asyncio\n",
    "# res = await regex_fun(floss_json_path, file_hash, root_dir, reprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7dd4c8dd-1ec1-481f-8103-2fe001323f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = process_censys_file(regex_json_path, vt_meta_file_path, file_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce9181e5-6ce1-41c9-88f9-14be18387806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16.0.0.0', '3.5.0.0', '4.0.0.0']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(rr.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d53422d7-e949-46b3-b735-2402cb3caf81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'16.0.0.0': {'IPData': {'ip': '16.0.0.0',\n",
       "   'services': [],\n",
       "   'location': {'continent': 'North America',\n",
       "    'country': 'United States',\n",
       "    'country_code': 'US',\n",
       "    'city': 'Palo Alto',\n",
       "    'postal_code': '94304',\n",
       "    'timezone': 'America/Los_Angeles',\n",
       "    'province': 'California',\n",
       "    'coordinates': {'latitude': 37.4334, 'longitude': -122.1842}},\n",
       "   'location_updated_at': '2024-03-31T22:52:28.450226806Z',\n",
       "   'whois': {'network': {'handle': 'HPE-15',\n",
       "     'name': 'HEWLETT PACKARD ENTERPRISE COMPANY'},\n",
       "    'organization': {'handle': 'HPE-15',\n",
       "     'name': 'HEWLETT PACKARD ENTERPRISE COMPANY',\n",
       "     'street': '3000 Hanover Street',\n",
       "     'city': 'Palo Alto',\n",
       "     'state': 'CA',\n",
       "     'postal_code': '94304',\n",
       "     'country': 'US',\n",
       "     'abuse_contacts': [{'handle': 'ABUSE7931-ARIN',\n",
       "       'name': 'Abuse reporting',\n",
       "       'email': 'abuse@hpe.com'}],\n",
       "     'admin_contacts': [{'handle': 'HPEIP-ARIN',\n",
       "       'name': 'HPE IPADDR',\n",
       "       'email': 'ddi-hostmaster@groups.ext.hpe.com'}],\n",
       "     'tech_contacts': [{'handle': 'HPEIP-ARIN',\n",
       "       'name': 'HPE IPADDR',\n",
       "       'email': 'ddi-hostmaster@groups.ext.hpe.com'}]}}}},\n",
       " '3.5.0.0': {'IPData': {'ip': '3.5.0.0',\n",
       "   'services': [],\n",
       "   'location': {'continent': 'North America',\n",
       "    'country': 'United States',\n",
       "    'country_code': 'US',\n",
       "    'city': 'Ashburn',\n",
       "    'postal_code': '20147',\n",
       "    'timezone': 'America/New_York',\n",
       "    'province': 'Virginia',\n",
       "    'coordinates': {'latitude': 39.04372, 'longitude': -77.48749}},\n",
       "   'location_updated_at': '2024-03-31T22:52:29.194422067Z',\n",
       "   'autonomous_system': {'asn': 14618,\n",
       "    'description': 'AMAZON-AES',\n",
       "    'bgp_prefix': '3.5.0.0/24',\n",
       "    'name': 'AMAZON-AES',\n",
       "    'country_code': 'US'},\n",
       "   'autonomous_system_updated_at': '2024-03-31T22:52:29.194422067Z',\n",
       "   'whois': {'network': {'handle': 'AMAZON-S3',\n",
       "     'name': 'Amazon Data Services NoVa'},\n",
       "    'organization': {'handle': 'ADSN-1',\n",
       "     'name': 'Amazon Data Services NoVa',\n",
       "     'street': '13200 Woodland Park Road',\n",
       "     'city': 'Herndon',\n",
       "     'state': 'VA',\n",
       "     'postal_code': '20171',\n",
       "     'country': 'US',\n",
       "     'abuse_contacts': [{'handle': 'AEA8-ARIN',\n",
       "       'name': 'Amazon EC2 Abuse',\n",
       "       'email': 'abuse@amazonaws.com'}],\n",
       "     'admin_contacts': [{'handle': 'IPMAN40-ARIN',\n",
       "       'name': 'IP Management',\n",
       "       'email': 'ipmanagement@amazon.com'}],\n",
       "     'tech_contacts': [{'handle': 'ANO24-ARIN',\n",
       "       'name': 'Amazon EC2 Network Operations',\n",
       "       'email': 'amzn-noc-contact@amazon.com'}]}}}},\n",
       " '4.0.0.0': {'IPData': {'ip': '4.0.0.0',\n",
       "   'services': [],\n",
       "   'location': {'continent': 'North America',\n",
       "    'country': 'United States',\n",
       "    'country_code': 'US',\n",
       "    'city': 'Secaucus',\n",
       "    'postal_code': '07094',\n",
       "    'timezone': 'America/New_York',\n",
       "    'province': 'New Jersey',\n",
       "    'coordinates': {'latitude': 40.78955, 'longitude': -74.05653}},\n",
       "   'location_updated_at': '2024-03-31T22:52:29.573658932Z',\n",
       "   'autonomous_system': {'asn': 3356,\n",
       "    'description': 'LEVEL3',\n",
       "    'bgp_prefix': '4.0.0.0/9',\n",
       "    'name': 'LEVEL3',\n",
       "    'country_code': 'US'},\n",
       "   'autonomous_system_updated_at': '2024-03-31T22:52:29.573658932Z',\n",
       "   'whois': {'network': {'handle': 'LVLT-ORG-4-8',\n",
       "     'name': 'Level 3 Parent, LLC'},\n",
       "    'organization': {'handle': 'LPL-141',\n",
       "     'name': 'Level 3 Parent, LLC',\n",
       "     'street': '100 CenturyLink Drive',\n",
       "     'city': 'Monroe',\n",
       "     'state': 'LA',\n",
       "     'postal_code': '71203',\n",
       "     'country': 'US',\n",
       "     'abuse_contacts': [{'handle': 'LAC56-ARIN',\n",
       "       'name': 'L3 Abuse Contact',\n",
       "       'email': 'abuse@level3.com'}],\n",
       "     'admin_contacts': [{'handle': 'APL7-ARIN',\n",
       "       'name': 'ADMIN POC LVLT',\n",
       "       'email': 'ipadmin@centurylink.com'}],\n",
       "     'tech_contacts': [{'handle': 'APL7-ARIN',\n",
       "       'name': 'ADMIN POC LVLT',\n",
       "       'email': 'ipadmin@centurylink.com'}]}}}}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2081a-19a2-4d9a-9049-e43e16b2c0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
